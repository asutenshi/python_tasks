{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-of9NtRS6Mge"
   },
   "source": [
    "\n",
    "# Библиотека Prophet для прогнозирования временных рядов\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkUkQCeX6Mgl"
   },
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbq6PUSj6Mgl"
   },
   "source": [
    "Или __«машинное обучение»__ — множество методов для разработки алгоритмов, которые решают задачу на основании поиска закономерностей в данных.  \n",
    "  \n",
    "В компаниях очень много данных, и логично, что бизнес хочет их использовать. \n",
    "  \n",
    "__Наиболее частая задача — прогнозирование различных показателей:__\n",
    "- количество пользователей\n",
    "- выручка\n",
    "- вероятность того, что пользователь совершит какое-то действие (например, сделает покупку или, наоборот, перестанет пользоваться продуктом)\n",
    "- и т. д.\n",
    "  \n",
    "Очень популярен __анализ временных рядов__. Временные ряды — это последовательность значений в определенный момент времени или за определенный промежуток времени. __Например:__\n",
    "- количество покупок по дням\n",
    "- выручка по дням/неделям/месяцам\n",
    "- и т. д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYI6LojT6Mgm"
   },
   "source": [
    "# Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p-mxOvc6Mgm"
   },
   "source": [
    "Сеть из трех продуктовых магазинов хочет спрогнозировать общий объем выручки сети на январь 2021 года. В качестве данных для построения моделей компания предоставляет данные о выручке за период `2020.05.01 - 2020.12.31`. Прогноз должен быть по дням."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XgFD1WW6Mgm"
   },
   "source": [
    "### Как это сделать?\n",
    "\n",
    "1. Исследуем данные, которые нам предоставили\n",
    "2. Построим стратегию расчета прогнозной модели\n",
    "3. Сделаем машин лернинг\n",
    "\n",
    "### Подготовим нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OaGB4pIp6Mgn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings # это библиотека для управления warning'ами — то есть различными предупреждениями\n",
    "                # ниже мы с помощью нее игнорим лишние предупреждения\n",
    "                # мы можем это убрать, но где-то всплывали ворнинги, и чтобы они не раздражали и не вызывали вопросов, мы их отключим\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuhWAuBP6Mgm"
   },
   "source": [
    "### Графический анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFbjPZtJ6Mgn"
   },
   "source": [
    "Для начала посмотрим на данные — какие есть поля, объем данных и т. д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "executionInfo": {
     "elapsed": 1667,
     "status": "error",
     "timestamp": 1612458084810,
     "user": {
      "displayName": "Настя Дорожкина",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gin5NXAhfC9yZx4Oh9wa8al3ZeC0gPLDIGrIoPj=s64",
      "userId": "09735892989798859718"
     },
     "user_tz": -180
    },
    "id": "HZfKAb1B6Mgn",
    "outputId": "df7ddade-7ffe-4c14-9eae-6ad82c30f3cd"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'time_trand/revenue-01052020-31122020.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_revenue \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_trand/revenue-01052020-31122020.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Programs\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\Programs\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\Programs\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\Programs\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\Programs\\Anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'time_trand/revenue-01052020-31122020.csv'"
     ]
    }
   ],
   "source": [
    "data_revenue = pd.read_csv('time_trand/revenue-01052020-31122020.csv') # подключимся к данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "executionInfo": {
     "elapsed": 1398,
     "status": "error",
     "timestamp": 1612458085875,
     "user": {
      "displayName": "Настя Дорожкина",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gin5NXAhfC9yZx4Oh9wa8al3ZeC0gPLDIGrIoPj=s64",
      "userId": "09735892989798859718"
     },
     "user_tz": -180
    },
    "id": "GsuIwOWo6Mgn",
    "outputId": "19f628d0-43cf-4fe6-b19d-0b6ef6133d56"
   },
   "outputs": [],
   "source": [
    "data_revenue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMazgPdW6Mgn"
   },
   "outputs": [],
   "source": [
    "data_revenue.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2XFIULY6Mgo"
   },
   "source": [
    "В наших данных есть общая информация о выручке в каждом из трех филиалов за каждый день с 1 мая 2020 по 31 декабря 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuVXUqLC6Mgo"
   },
   "source": [
    "Наша задача — спрогнозировать общую выручку по всем трем филиалам, поэтому посмотрим на общие данные.\n",
    "\n",
    "Возьмем гистограмму, которая отдаст частотность разных показателей выручки. В сам график передадим сгруппированный датасет по датам и выручке, чтобы каждый день встречался единожды и мы считали сумму по трем филиалам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVXDfzb_6Mgo"
   },
   "outputs": [],
   "source": [
    "# суммарная выручка по всем филиалам\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Распределение общей выручки') \n",
    "plt.xlabel('Выручка') \n",
    "plt.ylabel('Количество дней') \n",
    "plt.grid() \n",
    "\n",
    "plt.hist(data_revenue.groupby('date')['revenue'].sum(), bins=50) \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK4NM8RZ6Mgo"
   },
   "source": [
    "Видим, что в общей выручке явно есть смесь. Попробуем построить более детаельную гистограмму выручки без группирования по дню. \n",
    "\n",
    "Теперь за каждый день у нас будет 3 показателя за 1, 2 и 3 филиал."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqw_R3_w6Mgo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5)) \n",
    "plt.title('Распределение общей выручки') \n",
    "plt.xlabel('Выручка') \n",
    "plt.ylabel('Количество дней') \n",
    "plt.grid() \n",
    "\n",
    "plt.hist(data_revenue['revenue'], bins=150) # можем увеличить втрое количество бинов, чтобы визуализация была точнее\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3e4vKC06Mgp"
   },
   "source": [
    "Очевидная смесь в данных. Посмотрим, решается ли это разделением на филиалы. Для этого построим гистограммы отдельно по филиалам \n",
    "\n",
    "с помощью метода `.unique` возьмем список уникальных филиалов и пройдем по нему циклом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqNEdqDW6Mgp"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5)) \n",
    "\n",
    "filial = pd.unique(data_revenue['filial']) # сделаем серию со списком филиалов\n",
    "\n",
    "for f in filial: # фильтр по филиалу\n",
    "    sns.distplot(data_revenue.loc[data_revenue['filial'] == f]['revenue'], label = f);\n",
    "    \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZd2k1iU6Mgp"
   },
   "source": [
    "Как видим, во многом смесь объясняет разделение на филиалы, но не до конца.  \n",
    "Посмотрим, как выглядят данные в виде временных рядов (в сумме и в разделении по филиалам)\n",
    "\n",
    "### Отрисуем временной ряд\n",
    "\n",
    "Используем объект `dates` из библиотеки `matplotlib`, чтобы управлять детальности отсечек по дате на оси графика.\n",
    "\n",
    "В настройках графика мы укажем с помощью объекта `.MonthLocator()`, что хотим отображать даты по месяцам, а не по дням. \n",
    "\n",
    "- запишем в переменную `locator` месячную настройку подписей\n",
    "\n",
    "- с помощью метода `.gca()` сообщим о намерении обратиться к конкретной оси `axis`, в нашем случае ось Х\n",
    "\n",
    "- применим к оси метод `set_major_locator()` с месячной настройкой, которая лежит в переменной \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqH_mu-h6Mgp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates #импортируем для локаторов\n",
    "\n",
    "# суммарная выручка\n",
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "plt.title('Общая динамика выручки') \n",
    "plt.xlabel('Дата') \n",
    "plt.ylabel('Выручка') \n",
    "plt.grid()\n",
    "locator = mdates.MonthLocator() # передадим, что хотим подписи помесячно\n",
    "X = plt.gca().xaxis # обращаемся к оси х\n",
    "X.set_major_locator(locator) # передадим в ось параметры локатора\n",
    "sns.lineplot(x = data_revenue.groupby('date')['revenue'].sum().index, # .index, чтобы взять дату по оси x\n",
    "             y = data_revenue.groupby('date')['revenue'].sum()\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь проделаем то же самое для всех филиалов, используя цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvpKkJrp6Mgp"
   },
   "outputs": [],
   "source": [
    "# выручка по филиалам\n",
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "plt.title('Динамика выручки по филиалам') \n",
    "plt.xlabel('Дата') \n",
    "plt.ylabel('Выручка') \n",
    "plt.grid()\n",
    "for f in pd.unique(data_revenue['filial']):\n",
    "    sns.lineplot( \n",
    "                 x = data_revenue.loc[data_revenue['filial'] == f]['date'], \n",
    "                 y = data_revenue.loc[data_revenue['filial'] == f]['revenue'], \n",
    "                 label = f);\n",
    "    \n",
    "locator = mdates.MonthLocator()\n",
    "X = plt.gca().xaxis\n",
    "X.set_major_locator(locator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0N6bTqh6Mgq"
   },
   "source": [
    "На основании графического анализа данных можно сделать следующие выводы и предположения:\n",
    "- в данных есть смесь, вызванная тем, что разные филиалы в среднем имеют разную выручку\n",
    "- в данных можно увидеть тренды (возрастающий тренд для филиалов 1 и 2 и убывающий тренд для филиала 3)\n",
    "- в данных есть смесь, вызванная скорее сезонными явлениями (на это указывает циклический характер временных рядов)\n",
    "\n",
    "Таким образом в качестве потенциальных гипотез, которые помогут улучшить качество прогнозной модели, стоит учесть:\n",
    "- разделение по филиалам\n",
    "- выделение линейного тренда\n",
    "- добавление учета сезонности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYEtfmhQ6Mgq"
   },
   "source": [
    "# Построение прогнозной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cr1Dvj-q6Mgq"
   },
   "source": [
    "## Prophet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1LJvnAg6Mgq"
   },
   "source": [
    "Prophet — очень популярная библиотека от компании Facebook для анализа и прогнозирования временных рядов. \n",
    "\n",
    "- просто настраиваются прогнозные модели\n",
    "- можно полностью довериться алгоритму и использовать его как «черный ящик»\n",
    "- можно очень детально настроить множество параметров, которые доступны в алгоритме  \n",
    "\n",
    "__Модель__ — это алгоритм или набор алгоритмов, которые строятся на основании данных и с помощью которого/которых мы сможем прогнозировать.\n",
    "\n",
    "__Посмотрим [документацию](https://facebook.github.io/prophet/docs/quick_start.html)__\n",
    "\n",
    "\n",
    "Для построения модели нам нужно привести данные к стандартному виду, и нам понадобятся данные обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4Zd3wOz6Mgq"
   },
   "source": [
    "## Обучающая/контрольная выборки \n",
    "\n",
    "В машинном обучении есть понятия «обучение с учителем» и «обучение без учителя». То есть имеются ли у нас ответы и подсказки, чтобы проверить, хорошо ли обучилась модель.\n",
    "\n",
    "Мы можем обучить нашу модель на данных с мая по ноябрь, а на декабрьских проверить, хорошо ли наша модель предсказывает. Разделим данные на обучающую и контрольную группы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JFT0yS76Mgq"
   },
   "source": [
    "<div style=\"display: flex; width: 100%; font-family: Arial\">\n",
    "<div style=\"width: 100%; margin-left: 0px\">\n",
    "  <div style=\"font-size: 20px; line-height: 30px\">Обучающие данные</div>\n",
    "\n",
    "  <div\n",
    "    style=\"\n",
    "      font-size: 14px;\n",
    "      line-height: 20px;\n",
    "      color: rgba(0, 0, 0, 0.48);\n",
    "      margin-top: 16px;\n",
    "    \"\n",
    "  >\n",
    "    Основа для обучения модели. \n",
    "      <br/>\n",
    "      Они выступают неким эталоном динамики и поведения данных.  \n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 100%; margin-left: 24px\">\n",
    "  <div style=\"font-size: 20px; line-height: 30px\">Контрольные данные</div>\n",
    "\n",
    "  <div\n",
    "    style=\"\n",
    "      font-size: 14px;\n",
    "      line-height: 20px;\n",
    "      color: rgba(0, 0, 0, 0.48);\n",
    "      margin-top: 16px;\n",
    "    \"\n",
    "  >\n",
    "    Нужны для проверки, как хорошо обучилась модель и на сколько точно она может предсказывать будущее. \n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 100%; margin-left: 24px\">\n",
    "  <div style=\"font-size: 20px; line-height: 30px\">Переобучение модели</div>\n",
    "\n",
    "  <div\n",
    "    style=\"\n",
    "      font-size: 14px;\n",
    "      line-height: 20px;\n",
    "      color: rgba(0, 0, 0, 0.48);\n",
    "      margin-top: 16px;\n",
    "    \"\n",
    "  >\n",
    "    Ситуация, когда модель дает качество в обучении, но низкое на контроле, взяв единичные особенности обучающей выборки как общее поведение данных.\n",
    "   \n",
    "  </div>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVtCEi9Y6Mgr"
   },
   "source": [
    "<img style='float:left' src=\"https://raw.githubusercontent.com/foobar167/articles/master/Machine_Learning/Brochure/data/Ris2.6-Nedoobucheniye-optimum-i-pereobucheniye-v-regressii.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4loMfV4w6Mgr"
   },
   "source": [
    "В нашем случае, как и договорились, разделим данные на обучение и контроль следующим образом:\n",
    "- данные с мая по ноябрь — обучение\n",
    "- данные за декабрь — контроль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6Kkb7lV6Mgs"
   },
   "outputs": [],
   "source": [
    "train = data_revenue.loc[data_revenue['date'] < '2020-12-01']\n",
    "test = data_revenue.loc[data_revenue['date'] >= '2020-12-01']\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClugVXG46Mgs"
   },
   "source": [
    "### Попробуем построить первую модель для общей выручки\n",
    "\n",
    "Для начала сгруппируем данные каждой выборки по дате с суммой выручки. Индекс сбросим, чтобы работать только с колонками.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oWiIQ326Mgs"
   },
   "outputs": [],
   "source": [
    "train_0 = train.groupby(['date'])['revenue'].sum().reset_index() # сгруппировали данные обучающей выборки\n",
    "test_0 = test.groupby(['date'])['revenue'].sum().reset_index() # сгруппировали данные контрольной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSYm9HTZ6Mgs"
   },
   "outputs": [],
   "source": [
    "train_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyahag9Y6Mgs"
   },
   "source": [
    "Установим и импортируем `Prophet()`. Несколько дополнительных способой установки можно найти в отдельном ноутбуке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXMzwF-m6Mgt"
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smTpEpDv6Mgt"
   },
   "source": [
    "Сначала попробуем построить прогнозную модель без указания каких бы то ни было параметров — пусть алгоритм действует на свое усмотрение.\n",
    "\n",
    "__Параметры__ — это настройки модели. Обученная модель фиксирует ряд настроек, и мы на этапе обучения можем подсказать ей, как настраиваться или оставить настройки  «по умолчанию» . \n",
    "\n",
    "Чтобы создать модель, мы используем \n",
    "1. объект `Prophet()` из библиотеки\n",
    "\n",
    "2. метод `.fit()` для подгонки модели\n",
    "3. правильно подготовленные данные\n",
    "\n",
    "\n",
    "Создадим `model`, присвоив ей объект `Prophet()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpl-FGYa6Mgt"
   },
   "outputs": [],
   "source": [
    "model = Prophet()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi2XHqQ36Mgt"
   },
   "source": [
    "`.fit()` — метод подгонки модели под заданные данные и параметры модели (как раз он и обучает модель по историческим данным)\n",
    "\n",
    "Нужно, чтобы в метод `.fit()` подавался датафрейм с нужными полями. Заглянем еще раз в [документацию.](https://facebook.github.io/prophet/docs/quick_start.html)\n",
    "- `ds` — временная гранула, в нашем случае день\n",
    "\n",
    "- `y` — значения, которые будем предсказывать, в нашем случае выручка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CLvNOqs6Mgt"
   },
   "outputs": [],
   "source": [
    "train_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переименуем столбцы в обучающем и тестовом датасетах, чтобы они подходили для использования методов Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq4ITXTL6Mgu"
   },
   "outputs": [],
   "source": [
    "train_0.columns = ['ds', 'y'] # переименовали столбцы\n",
    "\n",
    "test_0.columns = ['ds', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os5z5pfF6Mgu"
   },
   "outputs": [],
   "source": [
    "model.fit(train_0) # подогнали модель под наши данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmpEWMRy6Mgu"
   },
   "source": [
    "Как видим, алгоритм сам нам подсказывает, что он проигнорировал, выбирая параметры  \n",
    "\n",
    "1. `INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.`\n",
    "\n",
    "    - годовую сезонность (*для годовой сезонности нам нужно иметь данные минимум за два года, чтобы суметь использовать ее в модели*)\n",
    "\n",
    "\n",
    "2. `INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.`\n",
    "\n",
    "    - дневную сезонность (*дневная сезонность может использоваться в случае, если данные собираются по часам/минутам, в нашем случае данные представлены по дням*).\n",
    "\n",
    "\n",
    "Зато он обнаружил недельную сезонность и использовал его при настройке модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ayl1j1Ur6Mgu"
   },
   "source": [
    "## Качество модели\n",
    "\n",
    "Посмотрим на качество получившейся модели. Для этого нам необходимо задать горизонт прогнозирования (в нашем случае это месяц) и создать дата-фрейм с датами из «будущего», для которого модель потом будет строить прогноз.\n",
    "\n",
    "`make_future_dataframe()` — метод профета, который создает дата-фрейм с временным периодом будущего. В аргумент ему мы передаем `periods` и задаем количество элементов, на которые хотим получить прогноз. \n",
    "\n",
    "Так как наши данные имеют дневную гранулярность, прогноз мы также получим по дням от последней даты на количество дней, указанное в скобках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkC2u-t46Mgu"
   },
   "outputs": [],
   "source": [
    "future = model.make_future_dataframe(periods=31) # говорим профету сделать дата-фрейм на 31 день\n",
    "future.tail(31) # выводим 31 строку с конца"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKlioCSv6Mgu"
   },
   "source": [
    "Теперь можем построить прогноз методом __`.predict`__.\n",
    "\n",
    "Применим его к нашей модели и запишем в отдельную переменную.\n",
    "\n",
    "доверительный интервал по умолчанию 95%, это популярный стандарт, который вполне нас устраивает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBAjf_2a6Mgv"
   },
   "outputs": [],
   "source": [
    "forecast = model.predict(future)\n",
    "forecast.head() # возвращает много колонок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные поля в прогнозе следующие:\n",
    "- `ds` — дата прогноза\n",
    "\n",
    "- `yhat` — спрогнозированное значение\n",
    "\n",
    "- `yhat_lower` — нижняя граница доверительного интервала для прогноза\n",
    "- `yhat_upper` — верхняя граница доверительного интервала для прогноза\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFlieNv76Mgv"
   },
   "outputs": [],
   "source": [
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail() # оставим только нужные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e40EZqqZ6Mgv"
   },
   "source": [
    "Также с помощью метода `.plot()` прогноз можно построить на графике и посмотреть визуально его адекватность.\n",
    "\n",
    "Черные точки — наши данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLzfZg3y6Mgv"
   },
   "outputs": [],
   "source": [
    "model.plot(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXneCaZk6Mgv"
   },
   "source": [
    "Кроме того, Prophet позволяет также наглядно разложить ряд на основные компоненты — тренд и сезонность:\n",
    "\n",
    "`plot_components()` — возвращает несколько графиков, среди которых тренд и столько сезонностей, сколько он найдет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUvd4a4P6Mgv"
   },
   "source": [
    "Точка с запятой в конце строки позволяет не выводить лишнего. Обычно такая проблема возникает с графиками — выводится лишняя информация или даже дублирующиеся графики.\n",
    "\n",
    "`Suppress output\n",
    "Put a ‘;’ at the end of a line to suppress the printing of output. This is useful when doing calculations which generate long output you are not interested in seeing. It also keeps the object out of the output cache, so if you’re working with large temporary objects, they’ll be released from memory sooner.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-oX0xg86Mgv"
   },
   "outputs": [],
   "source": [
    "model.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-5ZwgUn6Mgw"
   },
   "source": [
    "Видим, что тренд у общей выручки убывающий и у нас есть четкая недельная сезонность:\n",
    "- наибольшее количество покупок люди совершают в субботу\n",
    "- чуть меньше в воскресенье\n",
    "- среди будних дней больше всего покупок в понедельник и вторник"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOBjmJDF6Mgw"
   },
   "source": [
    "## Ошибка прогноза\n",
    "\n",
    "Давайте посмотрим, какая получилась ошибка прогноза. По модели мы строим прогноз на тестовый период, затем сравниваем его с контрольными данными и считаем отклонение.\n",
    "\n",
    "### Как посчитать отклонение?\n",
    "\n",
    "Ошибку прогноза мы посчитаем с помощью функции `mean absolute error()` — среднее значение модулей отклонений прогноза от факта — из библиотеки `scikit-learn`:\n",
    "\n",
    "В аргументы он принимает серию предсказанных значений и серию с контрольными значениями.  \n",
    "\n",
    "Что под капотом у функции: из факта вычитаем прогноз, берем от этого модуль (положительное значение) и считаем среднее значение по всем модулям. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJJrvWpc6Mgw"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(forecast['yhat'].tail(31), test_0['y']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptT5EGZf6Mgx"
   },
   "source": [
    "Сходу не понять, сильное ли отклонение мы получили. Посмотрим в %.\n",
    "\n",
    "Для того чтобы посчитать `отклонение в процентах`, необходимо `модули отклонений прогноза от факта` разделить на `факт`, от полученных значений посчитать среднее арифметическое и умножить на 100.\n",
    "\n",
    "`np.mean()` — функция библиотеки NumPy считает среднее арифметическое\n",
    "\n",
    "`np.abs()` — функция библиотеки NumPy, которая вернет модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ytvd1WR6Mgx"
   },
   "outputs": [],
   "source": [
    "modul_dif = np.abs(forecast.tail(31).reset_index()['yhat'] - test_0['y']) # положим модули отклонений от прогноза в отдельную переменную\n",
    "\n",
    "dif = np.mean(modul_dif/test_0['y']) # считаем среднее арифметическое отклонение \n",
    "\n",
    "dif_percent = dif*100 # получаем процент\n",
    "\n",
    "dif_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или все то же самое можно записать в одну строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(forecast.tail(31).reset_index()['yhat'] - test_0['y'])/test_0['y'])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-nVI95-6Mgx"
   },
   "source": [
    "Попробуем теперь сделать такой же прогноз, но отдельно по филиалам и сравним результаты.  \n",
    "Для начала подготовим данные отдельно по филиалам, сначала данные для обучения:\n",
    "\n",
    "1. отфильтруем для каждого филиала нужные строки\n",
    "2. переименуем столбцы для работы с `Prophet`\n",
    "\n",
    "Мы переименовывали столбцы, меняя значения атрибута `columns`, теперь используем метод `rename()` — с ним вы тоже будете часто сталкиваться в чужом коде. Параметр `axis = 1` указывает, что переименовываем колонки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6A-ZjcI6Mgx"
   },
   "outputs": [],
   "source": [
    "train_1 = train.loc[train['filial'] == 1,['date','revenue']].rename({'date':'ds','revenue':'y'},axis=1) \n",
    "train_2 = train.loc[train['filial'] == 2,['date','revenue']].rename({'date':'ds','revenue':'y'},axis=1)\n",
    "train_3 = train.loc[train['filial'] == 3,['date','revenue']].rename({'date':'ds','revenue':'y'},axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZeH2eHG6Mgx"
   },
   "source": [
    "И то же самое проделаем для подготовки данных, чтобы проверить качества модели, только возьмем исходный тестовый датасет. В нем данные за декабрь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKag0itN6Mgx"
   },
   "outputs": [],
   "source": [
    "test_1 = test.loc[test['filial'] == 1,['date','revenue']].rename({'date':'ds','revenue':'y'},axis=1)\n",
    "test_2 = test.loc[test['filial'] == 2,['date','revenue']].rename({'date':'ds','revenue':'y'},axis=1)\n",
    "test_3 = test.loc[test['filial'] == 3,['date','revenue']].rename({'date':'ds','revenue':'y'},axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9qWtxaV6Mgx"
   },
   "source": [
    "Теперь строим три модели по разным данным. Для каждой модели заведем переменную, так как нам нужны три разные модели.\n",
    "\n",
    "1. Заведем 3 разные модели для 3 филиалов с помощью объекта `Prophet()`.\n",
    "2. Передадим для каждой модели обучающий датасет в метод `fit()`, чтобы подогнать модель под наши данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iVifHHK6Mgy"
   },
   "outputs": [],
   "source": [
    "m_1 = Prophet() # модель первого филиала\n",
    "m_1.fit(train_1) # подгоняем модель под наши данные\n",
    "\n",
    "# аналогичный код для двух других филиалов\n",
    "\n",
    "m_2 = Prophet()\n",
    "m_2.fit(train_2)\n",
    "\n",
    "m_3 = Prophet()\n",
    "m_3.fit(train_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tXvpO8H6Mgy"
   },
   "source": [
    "Теперь нам предстоит построить прогноз на тот же период для трех моделей.\n",
    "\n",
    "Применим к каждой модели метод `predict()`, передав ему датасет с «будущими» датами.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdW47Eav6Mgz"
   },
   "outputs": [],
   "source": [
    "forecast_1 = m_1.predict(future) # датасет с прогнозом для первого филиала\n",
    "forecast_2 = m_2.predict(future) # датасет с прогнозом для второго филиала\n",
    "forecast_3 = m_3.predict(future) # датасет с прогнозом для третьего филиала"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстро визуализируем получившиеся данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIheg3oX6Mgz"
   },
   "outputs": [],
   "source": [
    "m_1.plot(forecast_1); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBnSNw1r6Mgz"
   },
   "outputs": [],
   "source": [
    "m_2.plot(forecast_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvKkkHCK6Mgz"
   },
   "outputs": [],
   "source": [
    "m_3.plot(forecast_3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На графиках видно, что модель отдает слишком грубое предсказание. Попробуем скорректировать его с помощью более тонких настроек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq9rBmNb6Mgz"
   },
   "source": [
    "__Для начала посмотрим на ошибки отдельно для каждого филиала.__\n",
    "\n",
    "Для этого для каждого филиала передадим в функцию `mean_absolute_error()` серию предсказанных значений и серию контрольных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DW-hMXaG6Mgz"
   },
   "outputs": [],
   "source": [
    "error_forecast_1 = mean_absolute_error(forecast_1['yhat'].tail(31), test_1['y'])\n",
    "error_forecast_2 = mean_absolute_error(forecast_2['yhat'].tail(31), test_2['y'])\n",
    "error_forecast_3 = mean_absolute_error(forecast_3['yhat'].tail(31), test_3['y'])\n",
    "\n",
    "error_forecast_1, error_forecast_2, error_forecast_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Найдем процент ошибки для каждого филиала.__\n",
    "\n",
    "Передадим в функцию `np.abs()` разницу между предсказанными данными и данными из тестовой выборки. Не забудем про `.reset_index()`.\n",
    "\n",
    "Передадим в функцию `np.mean()` деление модулей разниц, которые получили только что, на данные тестовой выборки и умножим на 100, чтобы получить `%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AloAmsd16Mg0"
   },
   "outputs": [],
   "source": [
    "modul_dif_1 = np.abs(forecast_1.tail(31).reset_index()['yhat'] - test_1.reset_index()['y'])\n",
    "dif_percent_1 = np.mean(modul_dif_1/test_1.reset_index()['y'])*100\n",
    "\n",
    "modul_dif_2 = np.abs(forecast_2.tail(31).reset_index()['yhat'] - test_2.reset_index()['y'])\n",
    "dif_percent_2 = np.mean(modul_dif_2/test_2.reset_index()['y'])*100\n",
    "\n",
    "modul_dif_3 = np.abs(forecast_3.tail(31).reset_index()['yhat'] - test_3.reset_index()['y'])\n",
    "dif_percent_3 = np.mean(modul_dif_3/test_3.reset_index()['y'])*100\n",
    "\n",
    "dif_percent_1, dif_percent_2, dif_percent_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Zw5MdTx6Mg0"
   },
   "source": [
    "В разных филиалах очень разный процент ошибки, давайте попробуем улучшить прогноз для первого и третьего филиалов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMzvHVOI6Mg0"
   },
   "source": [
    "## Настройка параметров модели\n",
    "\n",
    "Мы запускали модель без параметров, давайте посмотрим, какие параметры модели можно задать. Рассмотрим основные:\n",
    "- __`growth`__ — тренд \n",
    "    - `linear` — линейный, то есть данные растут или убывают равномерно\n",
    "    \n",
    "    - `logistic` — логистический сложносочиненный микс динамики данных\n",
    "    \n",
    "    \n",
    "- __`holidays`__ — дата-фрейм с описанием праздников/выходных дней, чтобы они учитывались при построении прогноза\n",
    "\n",
    "\n",
    "- __`seasonality`__ — сезонность\n",
    "\n",
    "    - `weekly_seasonality` — недельная сезонность \n",
    "\n",
    "    - `daily_seasonality` — дневная сезонность \n",
    "\n",
    "    - `yearly_seasonality` — годовая сезонность \n",
    "    \n",
    "\n",
    "- __`seasonality_mode`__ \n",
    "    - `multiplicative` — мультипликативная \n",
    "    - `additive` — аддитивная\n",
    "    \n",
    "Аддитивную сезонность имеет смысл использовать, если амплитуда колебаний сезонности из года в год не меняется. Если амплитуда колебаний сезонности из года в год меняется (т. е. размах уменьшается или увеличивается), то используем мультипликативную сезонность.\n",
    "\n",
    "Вытащим тренд и обозначим, что у нас мультипликативная сезонность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtY5VrBk6Mg0"
   },
   "outputs": [],
   "source": [
    "m_1 = Prophet(growth = 'linear', weekly_seasonality = True, seasonality_mode='multiplicative')\n",
    "m_1.fit(train_1)\n",
    "\n",
    "m_2 = Prophet(growth = 'linear', weekly_seasonality = True, seasonality_mode='multiplicative')\n",
    "m_2.fit(train_2)\n",
    "\n",
    "m_3 = Prophet(growth = 'linear', weekly_seasonality = True, seasonality_mode='multiplicative')\n",
    "m_3.fit(train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdrfCozL6Mg0"
   },
   "outputs": [],
   "source": [
    "forecast_1 = m_1.predict(future)\n",
    "forecast_2 = m_2.predict(future)\n",
    "forecast_3 = m_3.predict(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Haqv5iqi6Mg0"
   },
   "source": [
    "__Сравним ошибки__\n",
    "\n",
    "В модели без параметров ошибки были: \n",
    "- 6270.750421944441\n",
    "- 1627.752566953797\n",
    "- 13210.181303922529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXpVx_vE6Mg0"
   },
   "outputs": [],
   "source": [
    "error_forecast_1 = mean_absolute_error(forecast_1['yhat'].tail(31), test_1['y'])\n",
    "error_forecast_2 = mean_absolute_error(forecast_2['yhat'].tail(31), test_2['y'])\n",
    "error_forecast_3 = mean_absolute_error(forecast_3['yhat'].tail(31), test_3['y'])\n",
    "\n",
    "error_forecast_1, error_forecast_2, error_forecast_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pViSocQ96Mg0"
   },
   "source": [
    "__И посчитаем в процентах, так гораздо надежнее__ \n",
    "\n",
    "Напомню, что было:\n",
    "- 6.7900069525078095\n",
    "- 1.7779055372059944\n",
    "- 26.902829147537332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcljiijN6Mg1"
   },
   "outputs": [],
   "source": [
    "modul_dif_1 = np.abs(forecast_1.tail(31).reset_index()['yhat'] - test_1.reset_index()['y'])\n",
    "dif_percent_1 = np.mean(modul_dif_1/test_1.reset_index()['y'])*100\n",
    "\n",
    "modul_dif_2 = np.abs(forecast_2.tail(31).reset_index()['yhat'] - test_2.reset_index()['y'])\n",
    "dif_percent_2 = np.mean(modul_dif_2/test_2.reset_index()['y'])*100\n",
    "\n",
    "modul_dif_3 = np.abs(forecast_3.tail(31).reset_index()['yhat'] - test_3.reset_index()['y'])\n",
    "dif_percent_3 = np.mean(modul_dif_3/test_3.reset_index()['y'])*100\n",
    "\n",
    "dif_percent_1, dif_percent_2, dif_percent_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrjWccoH6Mg1"
   },
   "outputs": [],
   "source": [
    "# хотя, датасайнтисты запишут скорее так:\n",
    "\n",
    "np.mean(np.abs(forecast_1.tail(31).reset_index()['yhat'] - test_1.reset_index()['y'])/test_1.reset_index()['y'])*100, \\\n",
    "np.mean(np.abs(forecast_2.tail(31).reset_index()['yhat'] - test_2.reset_index()['y'])/test_2.reset_index()['y'])*100, \\\n",
    "np.mean(np.abs(forecast_3.tail(31).reset_index()['yhat'] - test_3.reset_index()['y'])/test_3.reset_index()['y'])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9dlR2Z16Mg1"
   },
   "source": [
    "Теперь даже если мы сложим наши ожидаемые ошибки по трем филиалам, увидим, что общий прогноз стал гораздо точнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSfTm3aA6Mg1"
   },
   "outputs": [],
   "source": [
    "f1 = forecast_1.tail(31).reset_index()['yhat']\n",
    "f2 = forecast_2.tail(31).reset_index()['yhat']\n",
    "f3 = forecast_3.tail(31).reset_index()['yhat']\n",
    "\n",
    "total_forecast = f1 + f2 + f3\n",
    "\n",
    "mean_absolute_error(total_forecast, test_0['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk8bMldz6Mg1"
   },
   "source": [
    "Теперь, когда модель выбрана, надо перестроить модель по всему набору данных и сделать прогноз на январь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaCLiD1N6Mg1"
   },
   "outputs": [],
   "source": [
    "# готовим данные с мая по декабрь\n",
    "\n",
    "data_revenue_1 = data_revenue.loc[data_revenue['filial'] == 1].drop('filial', axis=1).rename({'date':'ds','revenue':'y'},axis=1)\n",
    "data_revenue_2 = data_revenue.loc[data_revenue['filial'] == 2].drop('filial', axis=1).rename({'date':'ds','revenue':'y'},axis=1)\n",
    "data_revenue_3 = data_revenue.loc[data_revenue['filial'] == 3,['date','revenue']].rename({'date':'ds','revenue':'y'},axis=1)\n",
    "\n",
    "# обучаем модель\n",
    "\n",
    "m_1 = Prophet(growth = 'linear', weekly_seasonality = True, seasonality_mode='multiplicative')\n",
    "m_1.fit(data_revenue_1)\n",
    "\n",
    "m_2 = Prophet(growth = 'linear', weekly_seasonality = True, seasonality_mode='multiplicative')\n",
    "m_2.fit(data_revenue_2)\n",
    "\n",
    "m_3 = Prophet(growth = 'linear', weekly_seasonality = True, seasonality_mode='multiplicative')\n",
    "m_3.fit(data_revenue_3)\n",
    "\n",
    "# создаем дата-фрейм на январь\n",
    "\n",
    "future = m_1.make_future_dataframe(periods=31) # говорим профету сделать дата-фрейм на 31 день\n",
    "\n",
    "# наполняем январь данными для каждого филиала\n",
    "\n",
    "forecast_1 = m_1.predict(future)\n",
    "forecast_2 = m_2.predict(future)\n",
    "forecast_3 = m_3.predict(future)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpjwI_QF6Mg1"
   },
   "source": [
    "\n",
    "Теперь мы хотим объединить данные по филиалам. Тут нам мог бы пригодиться `merge()`, но вспомним, что мы можем объединять только 2 таблицы, то есть придется делать две операции объединения. А если в таблице одинаковые колонки, то мы можем просто сложить дата-фреймы.\n",
    "\n",
    "Чтобы воспользоваться этой суперспособностью дата-фреймов, нам нужно, чтобы дата оказалась в индексе, ведь даты в `pandas` не складываются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oITwdA6C6Mg2"
   },
   "outputs": [],
   "source": [
    "# складываем выручку на каждый день по всем 3 филиалам\n",
    "\n",
    "f1 = forecast_1.tail(31)[['ds','yhat']].set_index('ds')\n",
    "f2 = forecast_2.tail(31)[['ds','yhat']].set_index('ds')\n",
    "f3 = forecast_3.tail(31)[['ds','yhat']].set_index('ds')\n",
    "\n",
    "total_forecast = (f1 + f2 + f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3uDzB-H6Mg2"
   },
   "outputs": [],
   "source": [
    "total_forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D68E742X6Mg2"
   },
   "source": [
    "Возьмем только нужные столбцы и переименуем их для заказчика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRjlcJI96Mg2"
   },
   "outputs": [],
   "source": [
    "total_forecast.reset_index(inplace=True)\n",
    "total_forecast[['ds', 'yhat']]\n",
    "total_forecast.columns = ['date', 'revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o69eGd36Mg2"
   },
   "source": [
    "Выгрузим данные в таблицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0Nq0TC06Mg2"
   },
   "outputs": [],
   "source": [
    "total_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6OMS5PM6Mg2"
   },
   "outputs": [],
   "source": [
    "total_forecast.to_csv('forecast_jan2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INcjwxkR6Mg2"
   },
   "source": [
    "\n",
    "## Спасибо за внимание\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Skills_Python_6.1_lesson.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
